{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom LSTM/RNN AI summer experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJn7cMAYuVB"
      },
      "source": [
        "# Welcome to AI Summer tutorial:Intuitive understanding of recurrent neural networks\n",
        "\n",
        "This eductional LSTM tutorial heavily borrows from the Pytorch example for time sequence prediction that can be found here: https://github.com/pytorch/examples/tree/master/time_sequence_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDhz_3AJY185"
      },
      "source": [
        "### Basic imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsO7InKfWzg7"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuMNr4_x8OR2"
      },
      "source": [
        "### Generate synthetic sin wave data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWUEulFS8R9n"
      },
      "source": [
        "np.random.seed(2)\n",
        "\n",
        "T = 20\n",
        "L = 1000\n",
        "N = 200\n",
        "\n",
        "x = np.empty((N, L), 'int64')\n",
        "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
        "data = np.sin(x / 1.0 / T).astype('float64')\n",
        "torch.save(data, open('traindata.pt', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vtu67XoZBbA"
      },
      "source": [
        "### Our humble implementation of LSTM cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz1VsclwW5tu"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LSTM_cell_AI_SUMMER(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple LSTM cell network for educational AI-summer purposes\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length=10, hidden_length=20):\n",
        "        super(LSTM_cell_AI_SUMMER, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # forget gate components\n",
        "        self.linear_forget_w1 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_forget_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_forget = nn.Sigmoid()\n",
        "\n",
        "        # input gate components\n",
        "        self.linear_gate_w2 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_gate = nn.Sigmoid()\n",
        "\n",
        "        # cell memory components\n",
        "        self.linear_gate_w3 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.activation_gate = nn.Tanh()\n",
        "\n",
        "        # out gate components\n",
        "        self.linear_gate_w4 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r4 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_hidden_out = nn.Sigmoid()\n",
        "\n",
        "        self.activation_final = nn.Tanh()\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        x = self.linear_forget_w1(x)\n",
        "        h = self.linear_forget_r1(h)\n",
        "        return self.sigmoid_forget(x + h)\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "        # Equation 1. input gate\n",
        "        x_temp = self.linear_gate_w2(x)\n",
        "        h_temp = self.linear_gate_r2(h)\n",
        "        return self.sigmoid_gate(x_temp + h_temp)\n",
        "    \n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        x = self.linear_gate_w3(x)\n",
        "        h = self.linear_gate_r3(h)\n",
        "\n",
        "        # new information part that will be injected in the new context\n",
        "        k = self.activation_gate(x + h)\n",
        "        g = k * i\n",
        "        \n",
        "        # forget old context/cell info\n",
        "        c = f * c_prev\n",
        "        # learn new context/cell info\n",
        "        c_next = g + c\n",
        "        return c_next\n",
        "\n",
        "    def out_gate(self, x, h):\n",
        "        x = self.linear_gate_w4(x)\n",
        "        h = self.linear_gate_r4(h)\n",
        "        return self.sigmoid_hidden_out(x + h)\n",
        "\n",
        "    def forward(self, x, tuple_in ):\n",
        "        (h, c_prev) = tuple_in\n",
        "        # Equation 1. input gate\n",
        "        i = self.input_gate(x, h)\n",
        "        \n",
        "        # Equation 2. forget gate  \n",
        "        f = self.forget(x, h)\n",
        "\n",
        "        # Equation 3. updating the cell memory\n",
        "        c_next = self.cell_memory_gate(i, f, x, h,c_prev)\n",
        "\n",
        "        # Equation 4. calculate the main output gate\n",
        "        o = self.out_gate(x, h)\n",
        "\n",
        "        # Equation 5. produce next hidden output\n",
        "        h_next = o * self.activation_final(c_next)\n",
        "\n",
        "        return h_next, c_next\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw3qlyjKZH7I"
      },
      "source": [
        "### Our more humble implementation of GRU cell\n",
        "\n",
        "We will descibr GRU in part 2 but can you play around with it, if you want!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WjohnX4XB9h"
      },
      "source": [
        "class GRU_cell_AI_SUMMER(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple GRU cell network for educational purposes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_length=10, hidden_length=20):\n",
        "        super(GRU_cell_AI_SUMMER, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # reset gate components\n",
        "        self.linear_reset_w1 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_reset_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=True)\n",
        "\n",
        "\n",
        "        self.linear_reset_w2 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_reset_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=True)\n",
        "        self.activation_1 = nn.Sigmoid()\n",
        "\n",
        "        # update gate components\n",
        "        self.linear_gate_w3 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=True)\n",
        "        self.activation_2 = nn.Sigmoid()\n",
        "\n",
        "        self.activation_3 = nn.Tanh()\n",
        "\n",
        "    def reset_gate(self, x, h):\n",
        "        x_1 = self.linear_reset_w1(x)\n",
        "        h_1 = self.linear_reset_r1(h)\n",
        "        # gate update\n",
        "        reset = self.activation_1(x_1 + h_1)\n",
        "        return reset\n",
        "\n",
        "    def update_gate(self, x, h):\n",
        "        x_2 = self.linear_reset_w2(x)\n",
        "        h_2 = self.linear_reset_r2(h)\n",
        "        z = self.activation_2( h_2 + x_2)\n",
        "        return z\n",
        "\n",
        "    \n",
        "    def update_component(self, x,h,r):\n",
        "        x_3 = self.linear_gate_w3(x)\n",
        "        h_3 = r * self.linear_gate_r3(h) \n",
        "        gate_update = self.activation_3(x_3+h_3)\n",
        "        return gate_update\n",
        "\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Equation 1. reset gate vector\n",
        "        r = self.reset_gate(x, h)\n",
        "\n",
        "        # Equation 2: the update gate - the shared update gate vector z\n",
        "        z = self.update_gate(x, h)\n",
        "\n",
        "        # Equation 3: The almost output component\n",
        "        n = self.update_component(x,h,r)\n",
        "\n",
        "        # Equation 4: the new hidden state\n",
        "        h_new = (1-z) * n  + z * h\n",
        "\n",
        "        return h_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMgHiLofZfsF"
      },
      "source": [
        "## Putting the cells together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xyt-Gx8XHtQ"
      },
      "source": [
        "class Sequence(nn.Module):\n",
        "    def __init__(self, LSTM=True, custom=True):\n",
        "        super(Sequence, self).__init__()\n",
        "        self.LSTM = LSTM\n",
        "\n",
        "        if LSTM:\n",
        "          if custom:\n",
        "            print(\"AI summer LSTM cell implementation...\")\n",
        "            self.rnn1 = LSTM_cell_AI_SUMMER(1, 51)\n",
        "            self.rnn2 = LSTM_cell_AI_SUMMER(51, 51)\n",
        "          else:\n",
        "            print(\"Official PyTorch LSTM cell implementation...\")\n",
        "            self.rnn1 = nn.LSTMCell(1, 51)\n",
        "            self.rnn2 = nn.LSTMCell(51, 51)\n",
        "        #GRU\n",
        "        else:\n",
        "          if custom:\n",
        "            print(\"AI summer GRU cell implementation...\")\n",
        "            self.rnn1 = GRU_cell_AI_SUMMER(1, 51)\n",
        "            self.rnn2 = GRU_cell_AI_SUMMER(51, 51)\n",
        "          else:\n",
        "            print(\"Official PyTorch GRU cell implementation...\")\n",
        "            self.rnn1 = nn.GRUCell(1, 51)\n",
        "            self.rnn2 = nn.GRUCell(51, 51)\n",
        "\n",
        "\n",
        "        self.linear = nn.Linear(51, 1)\n",
        "\n",
        "    def forward(self, input, future=0):\n",
        "        outputs = []\n",
        "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "\n",
        "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
        "\n",
        "            if self.LSTM:\n",
        "              h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
        "              h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
        "            else:\n",
        "              h_t = self.rnn1(input_t, h_t)\n",
        "              h_t2 = self.rnn2(h_t, h_t2)\n",
        "\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        \n",
        "        # if we should predict the future\n",
        "        for i in range(future):  \n",
        "            if self.LSTM:\n",
        "              h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
        "              h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
        "            else:\n",
        "              h_t = self.rnn1(input_t, h_t)\n",
        "              h_t2 = self.rnn2(h_t, h_t2)\n",
        "\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEstEw_uU9n2"
      },
      "source": [
        "## Train code (based on the Pytorch example for time sequence prediction)\n",
        "\n",
        "that can be found here: https://github.com/pytorch/examples/tree/master/time_sequence_prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA5kieEMXY4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b235e3cf-0811-409d-a712-8b8966461dbb"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # set random seed to 0\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    # load data and make training set\n",
        "    data = torch.load('traindata.pt')\n",
        "    input = torch.from_numpy(data[3:, :-1])\n",
        "    print(input.shape)\n",
        "    target = torch.from_numpy(data[3:, 1:])\n",
        "    test_input = torch.from_numpy(data[:3, :-1])\n",
        "    test_target = torch.from_numpy(data[:3, 1:])\n",
        "    \n",
        "    # build the model. LSTM=False means GRU cell\n",
        "    seq = Sequence(LSTM=False, custom=False)\n",
        "\n",
        "    seq.double()\n",
        "    criterion = nn.MSELoss()\n",
        "    # use LBFGS as optimizer since we can load the whole data to train\n",
        "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
        "    # begin to train\n",
        "    for i in range(20):\n",
        "        print('STEP: ', i)\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            out = seq(input)\n",
        "            loss = criterion(out, target)\n",
        "            print('loss:', loss.item())\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        # begin to predict, no need to track gradient here\n",
        "        with torch.no_grad():\n",
        "            future = 1000\n",
        "            pred = seq(test_input, future=future)\n",
        "            loss = criterion(pred[:, :-future], test_target)\n",
        "            print('test loss:', loss.item())\n",
        "            y = pred.detach().numpy()\n",
        "        # draw the result\n",
        "        plt.figure(figsize=(30, 10))\n",
        "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
        "        plt.xlabel('x', fontsize=20)\n",
        "        plt.ylabel('y', fontsize=20)\n",
        "        plt.xticks(fontsize=20)\n",
        "        plt.yticks(fontsize=20)\n",
        "\n",
        "\n",
        "        def draw(yi, color):\n",
        "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth=2.0)\n",
        "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth=2.0)\n",
        "\n",
        "\n",
        "        draw(y[0], 'r')\n",
        "        draw(y[1], 'g')\n",
        "        draw(y[2], 'b')\n",
        "        plt.savefig('predict%d.png' % i)\n",
        "        plt.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([197, 999])\n",
            "Official PyTorch GRU cell implementation...\n",
            "STEP:  0\n",
            "loss: 0.5807195987929341\n",
            "loss: 0.5649699093952129\n",
            "loss: 0.24198546858936107\n",
            "loss: 0.0944302852726061\n",
            "loss: 0.051327534589922924\n",
            "loss: 0.02101161965136913\n",
            "loss: 0.016411523248697463\n",
            "loss: 0.01598952169189038\n",
            "loss: 0.015794807805653375\n",
            "loss: 0.014985336230117854\n",
            "loss: 0.012692426740654432\n",
            "loss: 1.4265140146087163\n",
            "loss: 0.0028594309674932206\n",
            "loss: 0.0023394372026554417\n",
            "loss: 0.0021042863174616394\n",
            "loss: 0.0011197246630718694\n",
            "loss: 0.000708825333724906\n",
            "loss: 0.000466144590945207\n",
            "loss: 0.00035688415690670136\n",
            "loss: 0.00030396461713427033\n",
            "test loss: 0.00011721832936651905\n",
            "STEP:  1\n",
            "loss: 0.0002883784493790896\n",
            "loss: 0.00028636675159411374\n",
            "loss: 0.00028613032211181275\n",
            "loss: 0.00028600792057773813\n",
            "loss: 0.0002857441527989899\n",
            "loss: 0.000285154311143996\n",
            "loss: 0.0002838605067565902\n",
            "loss: 0.00028123348173386784\n",
            "loss: 0.0002766905434897489\n",
            "loss: 0.00027088839564471626\n",
            "loss: 0.0002652773944835773\n",
            "loss: 0.0002634394074013378\n",
            "loss: 0.00026306864163131013\n",
            "loss: 0.00026291203790282386\n",
            "loss: 0.0002627119204428778\n",
            "loss: 0.00026226966528737107\n",
            "loss: 0.0002612598990105061\n",
            "loss: 0.00025901458465165213\n",
            "loss: 0.0002544341240096146\n",
            "loss: 0.0002466339790275893\n",
            "test loss: 9.027012145738461e-05\n",
            "STEP:  2\n",
            "loss: 0.00023590602265696046\n",
            "loss: 0.00021790769503731705\n",
            "loss: 0.0001967430135662353\n",
            "loss: 0.0001822731882582963\n",
            "loss: 0.0001831852752698095\n",
            "loss: 0.00017952782131851221\n",
            "loss: 0.000179376452458435\n",
            "loss: 0.00017919323343704\n",
            "loss: 0.00017890869928554606\n",
            "loss: 0.00017819651884612656\n",
            "loss: 0.00017632972301225944\n",
            "loss: 0.00017257702798097267\n",
            "loss: 0.00016402052283128585\n",
            "loss: 0.00015055671750984054\n",
            "loss: 0.00014393128960877473\n",
            "loss: 0.00014869678015368184\n",
            "loss: 0.00013225958227925126\n",
            "loss: 0.00012728396225329127\n",
            "loss: 0.00012546528499347753\n",
            "loss: 0.00012417905931411705\n",
            "test loss: 6.546916880843695e-05\n",
            "STEP:  3\n",
            "loss: 0.00012300230728909083\n",
            "loss: 0.00012230476912264364\n",
            "loss: 0.00012182308935772515\n",
            "loss: 0.00012154871597626869\n",
            "loss: 0.00012127035548414612\n",
            "loss: 0.00012085022401720502\n",
            "loss: 0.00012020880644792002\n",
            "loss: 0.00011903512131323386\n",
            "loss: 0.00011674014903986827\n",
            "loss: 0.00011321306931348475\n",
            "loss: 0.0001073839464720603\n",
            "loss: 0.00010495624845170543\n",
            "loss: 0.00010095017484767262\n",
            "loss: 9.793939131827776e-05\n",
            "loss: 9.253054718202694e-05\n",
            "loss: 9.093207329047696e-05\n",
            "loss: 9.028571574643819e-05\n",
            "loss: 8.992401344159155e-05\n",
            "loss: 8.858973679520227e-05\n",
            "loss: 8.770123294728046e-05\n",
            "test loss: 3.588926438906635e-05\n",
            "STEP:  4\n",
            "loss: 8.6828803473691e-05\n",
            "loss: 8.521720874261863e-05\n",
            "loss: 8.456812237849992e-05\n",
            "loss: 8.41506793177579e-05\n",
            "loss: 8.395857681071653e-05\n",
            "loss: 8.389177947411556e-05\n",
            "loss: 8.383723823597766e-05\n",
            "loss: 8.38042574589891e-05\n",
            "loss: 8.375147494590374e-05\n",
            "loss: 8.355013046844186e-05\n",
            "loss: 8.326411230745885e-05\n",
            "loss: 8.274010068932214e-05\n",
            "loss: 8.102792281372182e-05\n",
            "loss: 9.901789618083784e-05\n",
            "loss: 7.582948705773905e-05\n",
            "loss: 7.334222074576203e-05\n",
            "loss: 6.38441884785283e-05\n",
            "loss: 5.499739222907095e-05\n",
            "loss: 8.303435493617978e-05\n",
            "loss: 4.605140487800186e-05\n",
            "test loss: 4.3238639804725674e-05\n",
            "STEP:  5\n",
            "loss: 6.23344775257089e-05\n",
            "loss: 3.940137639741055e-05\n",
            "loss: 3.5735272029542375e-05\n",
            "loss: 3.3627488544935445e-05\n",
            "loss: 3.303049714591868e-05\n",
            "loss: 3.15353650842042e-05\n",
            "loss: 3.136636482309869e-05\n",
            "loss: 3.119837470748088e-05\n",
            "loss: 3.1135894126524684e-05\n",
            "loss: 3.109414747910596e-05\n",
            "loss: 3.0925416877593866e-05\n",
            "loss: 3.0678193405364e-05\n",
            "loss: 3.04590675681758e-05\n",
            "loss: 2.9997780792245977e-05\n",
            "loss: 2.9303518331791344e-05\n",
            "loss: 2.707186160000563e-05\n",
            "loss: 2.4296603808817337e-05\n",
            "loss: 2.2484961722122044e-05\n",
            "loss: 2.069033945205674e-05\n",
            "loss: 1.97671510643826e-05\n",
            "test loss: 2.1120368058260662e-05\n",
            "STEP:  6\n",
            "loss: 2.0321999786970776e-05\n",
            "loss: 1.866573389510076e-05\n",
            "loss: 1.8356998202266282e-05\n",
            "loss: 1.812901376477116e-05\n",
            "loss: 1.7961972950418647e-05\n",
            "loss: 1.785338824343422e-05\n",
            "loss: 1.7781986309112713e-05\n",
            "loss: 1.7744409220775492e-05\n",
            "loss: 1.7723854165128286e-05\n",
            "loss: 1.7692371622572263e-05\n",
            "loss: 1.7630673936660315e-05\n",
            "loss: 1.7520466686990364e-05\n",
            "loss: 1.714241879763297e-05\n",
            "loss: 1.6626969271102595e-05\n",
            "loss: 1.633710753378445e-05\n",
            "loss: 1.596554804350614e-05\n",
            "loss: 1.5175855329952244e-05\n",
            "loss: 1.3448036153423758e-05\n",
            "loss: 1.1928084980994447e-05\n",
            "loss: 4.9583139029886897e-05\n",
            "test loss: 1.3115754780960005e-05\n",
            "STEP:  7\n",
            "loss: 1.1204548771966969e-05\n",
            "loss: 1.0055415400893243e-05\n",
            "loss: 9.817958537945256e-06\n",
            "loss: 9.666622314625372e-06\n",
            "loss: 9.489750622499182e-06\n",
            "loss: 9.13237141319688e-06\n",
            "loss: 8.342893702621399e-06\n",
            "loss: 2.6023627047336273e-05\n",
            "loss: 7.678515612305207e-06\n",
            "loss: 7.5656505649003326e-06\n",
            "loss: 7.4021282739907535e-06\n",
            "loss: 7.316751938674141e-06\n",
            "loss: 7.238558680235663e-06\n",
            "loss: 7.194664981626177e-06\n",
            "loss: 7.176780849332255e-06\n",
            "loss: 7.127476704771869e-06\n",
            "loss: 7.100385804758823e-06\n",
            "loss: 7.0585087092053856e-06\n",
            "loss: 7.02255386918899e-06\n",
            "loss: 7.009281927685812e-06\n",
            "test loss: 8.57158269422097e-06\n",
            "STEP:  8\n",
            "loss: 7.005253484601909e-06\n",
            "loss: 7.0020597530217944e-06\n",
            "loss: 6.9999171241013386e-06\n",
            "loss: 6.996659834046587e-06\n",
            "loss: 6.990854770268195e-06\n",
            "loss: 6.9786755266891334e-06\n",
            "loss: 6.95413905296438e-06\n",
            "loss: 6.9107105689514306e-06\n",
            "loss: 6.849726293176223e-06\n",
            "loss: 6.7754594907884945e-06\n",
            "loss: 6.7101550795195945e-06\n",
            "loss: 6.6758981022545326e-06\n",
            "loss: 6.7670388050017015e-06\n",
            "loss: 6.651552117321633e-06\n",
            "loss: 6.647178305328665e-06\n",
            "loss: 6.627387739454236e-06\n",
            "loss: 6.60864997732766e-06\n",
            "loss: 6.591900810891121e-06\n",
            "loss: 6.564460098597689e-06\n",
            "loss: 6.548654629010766e-06\n",
            "test loss: 8.737066143284075e-06\n",
            "STEP:  9\n",
            "loss: 6.5232955251046505e-06\n",
            "loss: 6.511852553525711e-06\n",
            "loss: 6.482347990442666e-06\n",
            "loss: 6.464267526447155e-06\n",
            "loss: 6.44795515792866e-06\n",
            "loss: 6.43840865310881e-06\n",
            "loss: 6.424001632489303e-06\n",
            "loss: 6.4070934785131684e-06\n",
            "loss: 6.397708459210403e-06\n",
            "loss: 6.395701788910258e-06\n",
            "loss: 6.388598388871378e-06\n",
            "loss: 6.387425380695366e-06\n",
            "loss: 6.385449595231679e-06\n",
            "loss: 6.381894612985781e-06\n",
            "loss: 6.3640875947257525e-06\n",
            "loss: 6.317162660248651e-06\n",
            "loss: 6.26066515601945e-06\n",
            "loss: 6.181506400537238e-06\n",
            "loss: 6.104423556216693e-06\n",
            "loss: 6.316621569998713e-06\n",
            "test loss: 8.815502017758541e-06\n",
            "STEP:  10\n",
            "loss: 6.061061605785023e-06\n",
            "loss: 6.0448125966398975e-06\n",
            "loss: 6.026498971313164e-06\n",
            "loss: 6.009719084292483e-06\n",
            "loss: 5.9960173302501015e-06\n",
            "loss: 5.993672060057127e-06\n",
            "loss: 5.991822879009067e-06\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  11\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  12\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  13\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  14\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  15\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  16\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  17\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  18\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n",
            "STEP:  19\n",
            "loss: 5.991076567721566e-06\n",
            "test loss: 9.161086372895196e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMrOc2C6DJlQ"
      },
      "source": [
        "## Zip files to download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drjk_eov-669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "0e54ed13-6c5e-4ff6-f509-ec6a4dc2c051"
      },
      "source": [
        "!zip archive.zip predict*.png"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: predict0.png (deflated 7%)\n",
            "  adding: predict10.png (deflated 7%)\n",
            "  adding: predict11.png (deflated 7%)\n",
            "  adding: predict12.png (deflated 7%)\n",
            "  adding: predict13.png (deflated 7%)\n",
            "  adding: predict14.png (deflated 7%)\n",
            "  adding: predict15.png (deflated 7%)\n",
            "  adding: predict16.png (deflated 7%)\n",
            "  adding: predict17.png (deflated 7%)\n",
            "  adding: predict18.png (deflated 7%)\n",
            "  adding: predict19.png (deflated 7%)\n",
            "  adding: predict1.png (deflated 7%)\n",
            "  adding: predict2.png (deflated 7%)\n",
            "  adding: predict3.png (deflated 7%)\n",
            "  adding: predict4.png (deflated 7%)\n",
            "  adding: predict5.png (deflated 7%)\n",
            "  adding: predict6.png (deflated 7%)\n",
            "  adding: predict7.png (deflated 7%)\n",
            "  adding: predict8.png (deflated 7%)\n",
            "  adding: predict9.png (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}